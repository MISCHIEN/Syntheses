
\chapter{Discretization methods}
\section{Finite difference method}
\wrapfig{9}{l}{5}{0.4}{ch1/1}
It is based on discrete representation of numerical solution consisting of values of the solution at the nodes of a Cartesian mesh of uniform spacing (\autoref{ch1/1}). The discretization will consist in estimating the partial derivatives appearing in the governing equations by some algebraic relations at the nodes. Because of the limitation of the mesh we cannot deal with curved geometries. To do so we need coordinate changes. In this type of mesh, each grid point can be identified by a set of indexes i, j, (k), this type of mesh is called \textbf{structured mesh}. The neighbors are implicitly given by the index identification. 

\subsection{Evaluation of derivatives by finite differences}
\subsubsection{Difference formulas for the derivative $\bm{\D u / \D x}$}
The value of a function $u(x)$ on a point of indexes i, j on the mesh is noted $u_{ij}$, for time dependent functions we use $u^n_i$ where $n$ denotes the time index and $i$ the space index. Let's estimate $(\D u / \D x)_{ij}$, by definition: 

\begin{equation}
\left.\frac{\D u }{\D x}\right| _{ij} = \left.\frac{\D u }{\D x}\right| _{x_0,y_0}= \lim _{\xi \rightarrow 0} \frac{u(x_0+\xi, y_0) - u(x_0,y_0)}{\xi}  
\end{equation}

By taking $\xi = \Delta x$ to fit our grid points we get: 

\begin{equation}
\left.\frac{\D u }{\D x}\right| _{ij} \approx \lim _{\Delta x \rightarrow 0} \frac{u(x_0+\Delta x, y_0) - u(x_0,y_0)}{\Delta x}= \frac{u_{i+1\, j} - u_{ij}}{\Delta x}  
\end{equation}

In order to find a systematic way of deriving the equations, let's build the Taylor expansion of $u(x,y)$ around the mesh point $ij$: 

\begin{equation}
\begin{aligned}
&u_{i+1 \, j} = u_{ij}+ \Delta x \left.\frac{\D u }{\D x}\right| _{ij}  +\frac{\Delta x^2}{2} \left.\frac{\D ^2u }{\D x^2}\right| _{ij}  + \frac{\Delta x^3}{6} \left.\frac{\D ^3u }{\D x^3}\right| _{\xi} \qquad x_0 \leq \xi \leq x_0 + \Delta x\\
&\Leftrightarrow \left.\frac{\D u }{\D x}\right| _{ij} = \frac{u_{i+1 \, j} - u_{ij}}{\Delta x} -\underbrace{\frac{\Delta x}{2} \left.\frac{\D ^2u }{\D x^2}\right| _{ij}  - \frac{\Delta x^2}{6} \left.\frac{\D ^3u }{\D x^3}\right| _{\xi}}_{\mbox{truncation error}} = \frac{u_{i+1 \, j} - u_{ij}}{\Delta x} + \mathcal{O}(\Delta x)
\end{aligned}
\end{equation}

Giving the \textbf{forward difference formula}. We have then a truncation error whose behavior is dominated by the first term when $\Delta x \rightarrow 0$, so that $TE = \mathcal{O}(\Delta x)$, meaning that there exists a bounded number $K$ such that $\Delta x < \epsilon \rightarrow |TE| < K\Delta x$. The truncation error is always in the form $TE = \mathcal{O}(\Delta x ^q)$ where $q$ is the order of accuracy. The forward finite difference approximation of $\frac{\D u }{\D x}$ is first order accurate since $q = 1$.\\

If the order of the method is larger, for example second order, it means that if $\Delta x\rightarrow 0$, after a certain $\Delta x _{crit}$ the truncation error goes to 0 faster than the truncation error of a lower order method. If the mesh is not finer than a critical value, this is not true. When we go higher than a second order it is not clear in practice if we have something better because increasing the order allows the use of larger mesh, but is computationally more expensive too.\\

The definition for the derivative is not unique, for instance the \textbf{backward difference formula}:

\begin{equation}
u_{i-1 \, j} = u_{ij}- \Delta x \left.\frac{\D u }{\D x}\right| _{ij}  +\frac{\Delta x^2}{2} \left.\frac{\D ^2u }{\D x^2}\right| _{ij}  + H.O.T \Leftrightarrow \left.\frac{\D u }{\D x}\right| _{ij} = \frac{u_{ij} - u_{i-1 \, j}}{\Delta x} + \mathcal{O}(\Delta x)
\end{equation}

We have an infinity of finite difference formula if make the linear combination of the two last expressions. For example, if we sum half of the two we get the \textbf{central finite difference formula}: 

\begin{equation}
 \left.\frac{\D u }{\D x}\right| _{ij} = \frac{u_{i+1\, j} - u_{i-1\, j}}{2\Delta x} +\mathcal{O}(\Delta x^2)
\end{equation}

We see that the central difference formula is more accurate than the others and involves the same mesh distance. We could thus get as high order as desired, but at the cost of increasing the number of neighboring grid points in the equation and thus the computational cost. 

\subsubsection{General method to obtain finite difference formulas}
\begin{itemize}
\item[•] Choose the stencil (set of points involved in the expression);
\item[•] write Taylor series expansion of all the points in the stencil around the point where the derivative is to be evaluated;
\item[•] write the finite difference formula as a linear combination of stencil point values and adjust the coefficients such that it approximates the derivative to be evaluated with the desired order of accuracy.
\end{itemize}

\exemple{
Let's compute the finite difference formula for $\D ^2u /\D x^2$ using $i-1\, j, ij$ and $i+1\, j$ (first step). The second step gives: 

\begin{equation}
\begin{aligned}
&u_{i+1\, j} = u_{ij}+ \Delta x \left.\frac{\D u }{\D x}\right| _{ij}  +\frac{\Delta x^2}{2} \left.\frac{\D ^2u }{\D x^2}\right| _{ij}  + \frac{\Delta x^3}{6} \left.\frac{\D ^3u }{\D x^3}\right| _{ij} + \frac{\Delta x^4}{24} \left.\frac{\D ^4u }{\D x^4}\right| _{ij} + H.O.T\\
&u_{ij} = u_{ij}\\
&u_{i-1\, j} = u_{ij}- \Delta x \left.\frac{\D u }{\D x}\right| _{ij}  +\frac{\Delta x^2}{2} \left.\frac{\D ^2u }{\D x^2}\right| _{ij}  - \frac{\Delta x^3}{6} \left.\frac{\D ^3u }{\D x^3}\right| _{ij} + \frac{\Delta x^4}{24} \left.\frac{\D ^4u }{\D x^4}\right| _{ij} + H.O.T
\end{aligned}
\end{equation}

The third step gives: 

\begin{equation}
\left. \frac{\D ^2 u}{\D x^2} \right|_{ij} = au_{i+1\, j} + b u_{ij} + c u_{i-1\, j} = (a+b+c)u_{ij}+ (a-c) \Delta x \left. \frac{\D u}{\D x} \right|_{ij} + (a+c) \left. \frac{\D ^2 u}{\D x^2} \right|_{ij} + H.O.T
\end{equation}
}

\begin{tabular}{|c}
	\begin{minipage}{\textwidth}
	Depending on the accuracy needed, establish a system of 3 equations of 3 variables and solve by imposing the value for the different terms. For example if we want to approximate exactly we should cancel all the terms except the second order derivative term. 
	\end{minipage}
	\end{tabular}

\ \\\\
We can repeat this method again and again to obtain various finite difference formulas. You can consult pages 14 and 15 of the syllabus to see the list, not useful, just know that we can express the mixed second derivatives like $\D ^2u/\D x\D y$ too. 

\subsubsection{Derivation of finite difference formulas using operators}
In order to make the writing more compact, let's introduce some operators:
\begin{center}
\begin{tabular}{cc|cc}
$E_x^{+1}u_{ij} = u_{i+1\, j}$ & Forward shift &  $E_x^{-1}u_{ij} = u_{i-1\, j}$ & Backward shift\\
$\delta_x^{+}u_{ij} = u_{i+1\, j} - u_{ij}$ & Forward difference & $\delta_x^{-}u_{ij} =  u_{ij} - u_{i-1\, j}$ & Backward difference \\
$\mu _x u_{ij} = \frac{1}{2} \left(u_{i+\frac{1}{2} \, j}+u_{i-\frac{1}{2} \, j}\right)$ & Averaging & $\delta _xu_{ij} = u_{i+\frac{1}{2} \, j}-u_{i-\frac{1}{2} \, j}$ & Centered difference\\
\end{tabular}
\end{center}

Another operator for the centered difference can be used: 

\begin{equation}
\bar{\delta}_x = \frac{1}{2}\left(\delta ^+_x + \delta ^-_x\right) \qquad \Rightarrow \bar{\delta}_x u_{ij} = \frac{1}{2} \left( u_{i+1\, j} - u_{i-1 \, j} \right) 
\end{equation}

All these operators are also valid for $y$ coordinate. The following relations are verified: 

\begin{equation}
\delta^+ = E^{+1}-1 \qquad \delta^- = 1-E^{-1} \qquad \bar{\delta} = \mu \delta = \delta \mu 
\end{equation}

It is easy to derive finite difference formulas with these operator. For example, the Taylor series expansion of $u(x)$ is: 

\begin{equation}
\begin{array}{c}
u(x+\Delta x) = u(x) + \Delta x\frac{\D u}{\D x} (x) + \frac{\Delta x^2}{2} \frac{\D^2u}{\D x^2}(x) + \dots \\
 \Leftrightarrow E u(x) = \left( 1 + \Delta x D + \frac{(\Delta x D)^2}{2} + \dots \right) u(x) \qquad \Leftrightarrow Eu(x) = \exp (\Delta x D) u(x)
 \end{array}
\end{equation}

where we clearly see the Taylor expansion of $\exp (\Delta x D)$ and where $D_x = \frac{\D}{\D x}$. We can then make the following manipulation: 

\begin{equation}
E = \exp (\Delta x \D) \leftrightarrow \ln (E \bm{+1 - 1}) = \ln(1 + \delta ^+) = \Delta x D \qquad \Rightarrow D = \frac{\ln (1 + \delta ^+)}{\Delta x}
\end{equation}

And finally if we make the Mac Laurin expansion of the logarithm: 

\begin{equation}
D = \frac{\delta ^+}{\Delta x} - \frac{{\delta ^+}^2}{2\Delta x} + \frac{{\delta ^+}^3}{3\Delta x} + \dots
\end{equation}

By keeping the first term we find the first order forward difference formula, by keeping the first two terms we find the second order one, and so on. 

\subsection{Finite difference formulas for partial differential equations}

There are two strategies to express equations: 
\begin{itemize}
\item[•] \textbf{Strategy 1}: simply assemble the finite difference formula for each individual derivative;
\item[•] \textbf{Strategy 2}: same strategy used to find the finite difference in many steps, select the stencil, Taylor expansion on each point of the stencil, write the FD formula as a linear combination of the stencil points values and select the coefficients. 
\end{itemize}
These methods can't be differentiated by using the trucation error.
The first method is the most used. For the Laplace equation $\frac{\D ^2 u}{\D x^2}+ \frac{\D ^2 u}{\D y^2} = 0$ we have: 

\begin{equation}
\left. \frac{\D^2 u}{\D x^2} \right|_{ij} = \frac{{\delta_x} ^2u_{ij}}{\Delta x^2} + \mathcal{O}(\Delta x^2) \qquad \left. \frac{\D^2 u}{\D y^2} \right|_{ij} = \frac{{\delta _y} ^2u_{ij}}{\Delta y^2} + \mathcal{O}(\Delta y^2)
\end{equation}

If we sum this up we get: 

\begin{equation}
\frac{{\delta _x} ^2u_{ij}}{\Delta x^2} + \frac{{\delta _y} ^2u_{ij}}{\Delta y^2} = 0 \qquad \Rightarrow \frac{u_{i+1 \, j} - 2u_{ij} + u_{i-1\, j}}{\Delta x ^2} + \frac{u_{i \, j+1} - 2u_{ij} + u_{i\, j-1}}{\Delta y ^2} = 0
\end{equation}

The equation can contain a first order derivative and there can thus exists several discretization (forward, backward, ...). 

\subsection{Arbitrary geometries - irregular meshes}
The method we have seen is very simple, we love it. But the expressions rapidly become very difficult when dealing with irregular meshes. In addition, the order of accuracy is lower when irregular meshes compared to the regular one with same size mesh. The formulas become intractable for more than 3 points.  We cannot only use uniform meshes for at least two reasons: \\

\begin{itemize}
\item[•] \textbf{Computational domain geometry:} when the boundary is curved, it is quasi impossible to use uniform rectangular mesh. On the aerofoil example below, one can see that the grid points not always intersect the nodes on the geometry.  
\item[•] \textbf{Presence of regions where the solution varies rapidly:} for example, in fluid mechanics, there are regions such as the boundary layer where the fluid properties vary more rapidly than anywhere else. It is thus interesting to have finer mesh there and larger mesh somewhere where we don't care. 
\end{itemize}

\minifig{ch1/2}{ch1/3}{0.3}{0.4}{0.25}{0.5}

To tackle these problems, one can use coordinate transformation as suggests \autoref{ch1/3}. One can thus first fit a certain geometry, but also achieve a local concentration of mesh points. There are two disadvantages to this: we transform the geometrical complexity into equation complexity, and it is very difficult to find these transformation (numerical methods needed). The good news are that in the numerical plane, the mesh is regular and numerical algorithms have high efficiency, the transformation will be discussed later.


\section{Finite volume method}
The main idea is to take advantage of conservation equations whose fundamental form is the \textbf{integral form}, we discretize the integral. The principle consist in the application of the control volume method (macroscopic balances) in a large scale. The classical example is to have a bent tube, the flow exerts a force on the elbow and we can easily estimate it by momentum balance. We just take several small volumes where to apply this. \\

The great advantage of this method is to use arbitrary polygons (2D) or polyhedra (3D) as control volume so that it offers great \textbf{flexibility}. Unlike the finite difference method, the finite volume method can accommodate arbitrary control volume shapes, this eases mesh generation dramatically. There are some independent variables (time and space) that does not need flexibility it is the time variable, this is why wee still use finite differences for time discretization. In addition, since the integral form is discretized, it allows the computation of \textbf{weak solutions} of the flow.

\subsection{Fundamental principles and variants of the method}
Let's consider the integral form of a general system of conservation equations: 

\begin{equation}
U = \left(
\begin{array}{c}
\rho \\
\rho \vec{u}\\
\rho E
\end{array}
 \right)
\qquad \Rightarrow \frac{\D U}{\D t} + \nabla . \vec{F} = Q,
\end{equation} 

where $\vec{F}$ is the \textbf{flux vector} and $Q$ the \textbf{source term}. If we take the momentum equation and the conservation equation, we can see that there is a part independent of the derivative of $U$ (convective term) and a diffusive term dependent of $\nabla U \rightarrow \vec{F} = \vec{F}(U,\nabla U)$: 

\begin{equation}
\frac{\D \rho \vec{u}}{\D t} + \nabla (\underbrace{\rho \vec{u}\otimes \vec{u}+ p \bar{\bar{1}}}_{\mbox{convective}}  - \underbrace{\bar{\bar{\tau}}}_{\mbox{diffusive}}) = \rho \vec{g}
\end{equation}

The corresponding integral form is the basic original form obtained by integration of the equation over a control volume $\Omega$: 

\begin{equation}
\frac{d}{dt}\int _\Omega U d\Omega + \oint _{\D \Omega} \vec{F}.\vec{n}\, dS = \int _{\Omega} Q \, d\Omega
\end{equation}

Remark that discontinuities are allowed in this integral form since we do not have to verify the differentiation everywhere in the domain. If we subdivide the domain in elementary volumes and use the average value of $U$ on that volumes $\int _{\Omega _k}U\, d\Omega = U_k \Omega _k$ these are chosen as the parameters of the discrete representation, and assume the control volume to be a polygon ($\Gamma _m$ the faces), we have: 

\begin{equation}
\frac{d}{dt}(\Omega _k U_k) + \sum _{\Gamma _m \in \D \Omega _k} \int _{\Gamma _m} \vec{F}.\vec{n}\, dS = \int _{\Omega _k}Q\, d\Omega
\end{equation}

To make the discretization, we need to evaluate the remaining surface and volume integrals in terms of neighboring control volume averages. How to build the control volumes? First a mesh of non-overlapping elementary surfaces/volumes is generated, these are called cells. The design of control volumes must respect a certain number of conditions: \\

\begin{itemize}
\item[•] the union of CVs must cover the whole domain of interest; 
\item[•] the CVs may overlap but the boundaries of a CV should be either lying on the domain boundary or belong to the boundary of another CV. Each CV boundary must be shared by two CVs;
\item[•] the expression of the flux integral on a common edge should be the same for the two CVs it belongs to. \\
\end{itemize}

Consider two CVs K and L with a common face $\Gamma _c$ and make the sum: 

\begin{equation}
\begin{aligned}
&\frac{d}{dt}(\Omega _K U_K) + \sum _{\Gamma _m \in \D \Omega _K} \int _{\Gamma _m} \vec{F}.\vec{n}\, dS = \int _{\Omega _K}Q\, d\Omega \\
 &\frac{d}{dt}(\Omega _L U_L) + \sum _{\Gamma _m \in \D \Omega _L} \int _{\Gamma _m} \vec{F}.\vec{n}\, dS = \int _{\Omega _L}Q\, d\Omega\\
 \Rightarrow \frac{d}{dt}(\Omega _K &U_K + \Omega _L U_L) + \sum _{\Gamma _m \in \D \Omega _K \cup\, \D\Omega _L \setminus \Gamma _c} \int _{\Gamma _m} \vec{F}.\vec{n}\, dS = \int _{\Omega _K\cup \Omega _L}Q\, d\Omega 
\end{aligned}
\end{equation}

where we can observe that the common boundary integral disappears since the flux should be the same but the normals are opposite to each others. This last property is called \textbf{telescopic property} that ensures the conservation at the discrete level and the capture of discontinuities. Indeed, if the flux was different on K and L, the term would remain. \\

\wrapfig{11}{l}{2.8}{0.3}{ch1/4}
Several arrangement methods exists: 
\begin{itemize}
\item[•] the CV coincide with the mesh cell $\rightarrow$ cell-centered method;
\item[•] the CV is made out of mesh cells having a common vertex $\rightarrow$ cell-vertex method;
\item[•] the CV is made out of part of mesh cells sharing a common vertex $\rightarrow$ vertex centered method. \\
\end{itemize}

In the two last ones, it is common to associate the volume average to the corresponding vertex as done in finite difference. Only these will be considered. Let's mention that it is not compulsory to use the same CVs for different equations of a system of equations. 

\subsection{Evaluation of fluxes through faces}
In general we will approximate the integral over a length/surface $Gamma _i$ by a one point quadrature integration formula: 

\begin{equation}
\int _{\Gamma _m} \vec{F}.\vec{n} \, dS \approx \vec{F}_m .\vec{n}_m S_m
\end{equation}

This is sufficient for first order and second order methods, for higher order you have to use more points quadrature. Moreover, higher order are not easy to construct, this is why we have finite element methods. Let's discuss about aerothermodynamic problems ($\vec{F} = \vec{F} (U, \nabla U)$) in 1D for simplicity. We will consider a vertex-centered 1D FV method and the CVs are segments. The discretization becomes: 

\begin{equation}
\frac{d}{dt} (\Delta x_i U_i) + F_{i+\frac{1}{2}} - F_{i-\frac{1}{2}} = 0
\label{1.21}
\end{equation}

How to express the $F_{i+1/2}$ and the other in function of cell averages? One needs to specify a \textbf{numerical flux formula} which plays the same role as finite difference formulas in finite difference method. We will say that: 

\begin{equation}
F_{i+\frac{1}{2}} \approx \Phi (U_{i-k+1}, \dots , U_{i+k})
\end{equation}

For the method to be at least of order one, the approximation should be exact for a uniform field $\Phi (U,\dots , U) = F(U)$. The simplest choice is to take an arithmetic average of the fluxes or of the variables: 

\begin{equation}
\Phi (U_i, U_{i+1}) = (F_i + F_{i+1})/2 \qquad \Phi (U_i , U_{i+1}) = F\left(\frac{U_i + U_{i+1}}{2}\right)
\end{equation}

This applied to \autoref{1.21} gives: 

\begin{equation}
\frac{d}{dt}(\Delta x_i U_i) + \frac{F_{i} + F_{i+1}}{2} - \frac{F_{i-1} + F_i}{2} = 0 \qquad \Rightarrow \frac{dU_i}{dt} + \frac{F_{i+1} - F_{i-1}}{2\Delta x_i} = 0
\end{equation}

which is the same expression as obtained by central finite difference formula. One can retrieve the first order forward and backward finite difference formula by choosing $\Phi (U_i,U_{i+1}) = F(U_{i+1})$ and $\Phi (U_i,U_{i+1}) = F(U_i)$. \\

Let's come back to the nature of the finite volume numerical representation. It consists of a set of average values over subdomains and is thus clearly a discrete representation. For cell-centered or vertex-centered methods (not overlapping) it is easy to reconstruct a functional representation out of the averages. A piecewise constant reconstruction and a linear reconstruction are illustrated below. 

\minifig{ch1/5}{ch1/6}{0.5}{0.5}{0.35}{0.35}

In the second, the solution gradient is estimated in each CV but the reconstruction remains discontinuous at the boundaries so that it does not eliminate the need for a numerical flux function to compute the flux across the boundaries. But it allows to easily construct more accurate flux functions, starting from a two variable flux function: 

\begin{equation}
F_{i+\frac{1}{2}} \approx \Phi (U_i , U_{i+1})
\end{equation}

associated with the constant reconstruction, one obtains more accuracy by replacing $U_i$ and $U_{i+1}$ by $U_L$ and $U_R$. For instance using the backward flux formula $\Phi (U_L,U_R) = F(U_L)$ and a gradient estimation in CV i based on the back point:

\begin{equation}
\left(\frac{\D U}{\D x} \right)_i \approx \frac{U_i - U_{i-1}}{\Delta x} \qquad \Rightarrow U_{L, i+\frac{1}{2}} = \frac{3}{2} U_i -\frac{1}{2}U_{i-1},
\end{equation}

One can obtain the following space discretization: 

\begin{equation}
\frac{dU_i}{dt} + \frac{1}{\Delta x} \left( F\left(\frac{3U_i - U_{i-1}}{2} \right) - F\left(\frac{3U_{i-1} - U_{i-2}}{2} \right) \right) = 0
\end{equation}

and for a particular $F(U) = au$, we have: 

\begin{equation}
\frac{dU_i}{dt} + a \frac{3U_i - 4U_{i-1}+U_{i-2}}{2\Delta x}
\end{equation}

which is the one we found in previous section. Generally, for a polynomial reconstruction of order $k$ we shall obtain a discretization of order at least $k+1$. For the diffusive fluxes we have to estimate the gradient of variables on the faces which can be done directly or by averaging the estimated gradients in the two neighboring CVs. This is done by Green-Gauss theorem: 

\begin{equation}
\int _\Omega \nabla U \, d\Omega = \oint _\Gamma U\vec{n}\, dS
\end{equation}

by choosing an auxiliary control volume $\Omega$ centered on the point where one wishes to estimate the gradient. 

\subsection{Vertex-centered finite volumes in two dimensions and comparison with finite differences in transformed coordinates}
\wrapfig{8}{l}{3.5}{0.3}{ch1/7}
Here we will show that the FV method is equivalent to the FD in transformed coordinates. We consider first a vertex centered FV method on a structured mesh and then FD method in transformed coordinates. The advantage of structured mesh is the explicit connectivity, but its generation is more complex. We take the finite volume around the point $i, j$. Let's write the Euler equation: 

\begin{equation}
\frac{\D U}{\D t} +\frac{\D F_x}{\D x} + \frac{\D F_y}{\D y} = \frac{\D U}{\D t } + \nabla .\vec{F} = 0
\end{equation}

The finite volume discretization over the 4 points can be written as: 

\begin{equation}
\Omega _{ABCD} \frac{dU_{ij}}{dt} + \sum_{m=1}^{4} \vec{F}_m\vec{n}_ml_m=0
\end{equation} 

where $\vec{F}_m$ can be chosen as the average value (middle of sides) and $n_x l = \Delta y, n_y l =  - \Delta x$ since if we consider the angle $\theta$ made by AB we have that $\vec{n} = \cos \theta \vec{e}_1 + \sin \theta \vec{e}_2$ with $\cos \theta = \frac{\Delta y_{AB}}{AB}$ and $\sin \theta = -\frac{\Delta x_{AB}}{AB}$. $\Omega _{ABCD}$ can be computed as the vector product of diagonals: 

\begin{equation}
\Omega _{ABCD} = \frac{1}{2}|\Delta \vec{x} _{AC} \times \Delta \vec{x}_{BD}| = \frac{1}{2} (\Delta x_{AC}\Delta y_{BD}-\Delta x_{BD}\Delta y_{AC})
\end{equation}

We see that the remaining work to discretize is to express $\frac{dU_{ij}}{dt}$ by a finite difference formula. We could do in other way. \\

Let's now examine the transformed coordinates FD, let's do the chain rule and replace the expressions in Euler equation to get the transformed coordinates equation: 

\begin{equation}
\frac{\D F_x}{\D x} = \xi _x\frac{\D F_x}{\D \xi} + \eta _x\frac{\D F_x}{\D \eta}\qquad \frac{\D F_y}{\D y} = \xi _y\frac{\D F_y}{\D \xi} + \eta _y\frac{\D F_y}{\D \eta}
\label{1.33}
\end{equation}

$\xi _x, \xi _y, \eta _x \eta _y$ are the \textbf{metric terms}. We have the relation $\frac{dx}{d\xi} \frac{d\xi}{dx} = 1$ that can be generalized into matrix as follows: 

\begin{equation}
\left( 
\begin{array}{cc}
x _\xi & x_\eta\\
y _\xi & y_\eta
\end{array}
\right)
\left( 
\begin{array}{cc}
\xi _x & \xi _y\\
\eta _x & \eta _y
\end{array}
\right)
=
\left( 
\begin{array}{cc}
1 & 0\\
0 & 1
\end{array}
\right)
\end{equation}

We can express the following relations: 

\begin{equation}
\begin{array}{c}
\xi _x = \frac{\left| 
\begin{array}{cc}
1 & x_\eta\\
0 & y_\eta
\end{array}
\right|}{\left|
\begin{array}{cc}
x _\xi & x_\eta\\
y _\xi & y_\eta
\end{array}
\right|}
\qquad \Rightarrow J\xi _x = y_\eta \quad J\xi _y = -x_\eta \quad J\eta _x = -y_\xi \quad J\eta _y = x_\xi
\\
\Rightarrow \frac{\D (J\xi _x)}{\D \xi} + \frac{\D (J\eta_x)}{\D \eta} = 0 \qquad \frac{\D (J\xi _y)}{\D \xi} + \frac{\D (J\eta_y)}{\D \eta} = 0
\end{array}
\end{equation}

We can multiply by J \eqref{1.33} and find: 

\begin{equation}
\begin{aligned}
&J\frac{\D F_x}{\D x} = J\xi _x\frac{\D F_x}{\D \xi} + J\eta _x\frac{\D F_x}{\D \eta} = \frac{\D (J\xi _x F_x)}{\D \xi} + \frac{\D (J\eta _xF_x)}{\D \eta} - F_x\underbrace{ \left( \frac{\D (J\xi _x)}{\D \xi} + \frac{\D (J\eta _x)}{\D \eta} \right)}_{=0}
\\
&J\frac{\D F_y}{\D y} = J\xi _y\frac{\D F_y}{\D \xi} + J\eta _y\frac{\D F_y}{\D \eta} = \frac{\D (J\xi _y F_y)}{\D \xi} + \frac{\D (J\eta _yF_y)}{\D \eta} - F_y\underbrace{ \left( \frac{\D (J\xi _y)}{\D \xi} + \frac{\D (J\eta _y)}{\D \eta} \right)}_{=0}
\end{aligned}
\end{equation}

So that by multiplying the transformed Euler equation (conservative) by J and replacing we get: 

\begin{equation}
J\frac{\D U}{\D t} + \frac{\D (J\xi _x F_x + J\xi _y F_y)}{\D \xi} + \frac{\D (J\eta _xF_x + J\eta _yF_y)}{\D \eta} = 0
\end{equation}

We can now discretize this equation using centered finite differences: 

\begin{equation}
\begin{aligned}
J\frac{\D U_{ij}}{\D t} &+ \frac{(J\xi _x F_x + J\xi _y F_y)_{i+\frac{1}{2}\, j}-(J\xi _x F_x + J\xi _y F_y)_{i-\frac{1}{2}\, j}}{\Delta \xi}\\ 
&+ \frac{(J\eta _xF_x + J\eta _yF_y)_{i\, j+\frac{1}{2}} - (J\eta _xF_x + J\eta _yF_y)_{i\, j-\frac{1}{2}}}{\Delta \eta} = 0
\end{aligned}
\end{equation}

where $\Delta \xi = \Delta \eta = 1$. We can easily rewrite this by defining the middle point as average: 

\begin{equation}
\begin{aligned}
J\frac{\D U_{ij}}{\D t} &+ (J \nabla \xi)_{i+\frac{1}{2}\, j} \frac{\vec{F}_{ij}+ \vec{F}_{i+1\, j}}{2} - (J \nabla \xi)_{i-\frac{1}{2}\, j} \frac{\vec{F}_{i-1 \, j}+ \vec{F}_{ij}}{2} \\
&+ (J \nabla \eta)_{i \, j + \frac{1}{2}} \frac{\vec{F}_{ij}+ \vec{F}_{i\, j + 1}}{2}
- (J \nabla \eta)_{i \, j -\frac{1}{2}} \frac{\vec{F}_{i\, j-1}+ \vec{F}_{ij}}{2} = 0
\end{aligned}
\end{equation}

We can express the derivatives of the metric terms easily, for example for: 

\begin{equation}
\begin{aligned}
&(J\xi _x)_{i+\frac{1}{2}\, j} = (y_\eta) _{i+\frac{1}{2}\, j} = \frac{y_B - y_A}{\Delta \eta} = \Delta y_{AB}\\
&(J\xi _y)_{i+\frac{1}{2}\, j} = (-x_\eta) _{i+\frac{1}{2}\, j} = \frac{-x_B + x_A}{\Delta \eta} = -\Delta x_{AB}
\end{aligned}
\end{equation}

We found out that $(J\nabla \xi)_{i+\frac{1}{2}\, j} = (\vec{n} l)_{AB}$ and we can do the same for the others. To conclude, we have to proof that $J = \Omega _{ij}$ but this is obvious by computing the determinant and we know the expression of $\vec{x}_\xi$ and $\vec{x}_\eta$ at point $i\pm \frac{1}{2} \, j$ and $i\, j \pm\frac{1}{2}$ so we just have to make the average to have $ij$: 

\begin{equation}
J_{ij} = |\vec{x}_\xi \times \vec{x}_\eta |_{ij} = \frac{1}{2}(\Delta \vec{x}_{DA}-\Delta \vec{x}_{BC}\times \Delta \vec{x}_{AB}- \Delta \vec{x}_{CD})
\end{equation}

We can conclude that FV is the generalization of FD in changed coordinate. The advantage of the finite volume is that we can now deal with any quadrilateral. 

\section{Finite element method}
It is based on a functional representation, a linear combination of basis or \textbf{shape functions} $u*(x) = \sum _{i=1}^n a_iv_i(x)$ and the parameters are the \textbf{coefficients} of the basis functions, that are determine such that we get the best approximation of the exact solution. It differs from Galerkin and Ritz method by the selection of a \textbf{piecewise polynomial interpolation function} as shape functions. It is a method to construct generalized finite difference formulas. For example, the numerical solution parameters are values of the solution at particular points (nodes) and the discretized equations have a local character as in FD. \\

The convergence of the method is primordial, the numerical solution must tend to the exact solution when the number of parameters tends to infinity.

\subsection{Various form of a differential equation}
We already saw that the conservation laws can be put under an integral form. But other forms exist, for example the \textbf{weak form} and the \textbf{variational form}. \\

The \textbf{differential form}, also called strong form (needs to be satisfied on all points of the domain and thus the derivatives too). Then we have the \textbf{integral form} developed by physicians and that does not require to be differentiable everywhere and thus allows the existence of discontinuities: \textbf{weak solutions}. \\

To illustrate the weak form, consider the elastic deformation of a bar fixed at his upper end and submitted to its own weight and a traction force at the other end. Taking x-axis pointing downward, the strong form is: 

\begin{equation}
\frac{d}{dx} \left( k\frac{du}{dx} \right) + \mu g = 0 \qquad u(0) = 0 \quad k\frac{du}{dx}(L) - F = 0
\end{equation} 

where u is the displacement, k the rigidity (can be discontinuous) and $\mu$ the mass per unit length. As the equation must be satisfied everywhere, we can integrate the equation after having multiplied by a test function and add a null term due to the boundary condition: 

\begin{equation}
-\int _0^L \nu (x) \left[ \frac{d}{dx} \left( k\frac{du}{dx} \right)+ \mu g\right]\, dx + \nu (L) \left[ k \frac{du}{dx}(L) - F \right] = 0
\end{equation}

And by integrating by parts $f = \nu (x)$ and $g' = \frac{d}{dx}(k\frac{du}{dx})$: 

\begin{equation}
\int _0 ^L \left[\frac{d\nu}{dx} k \frac{du}{dx} - \mu g \nu \right]\, dx - \underbrace{\nu (0)k\frac{du}{dx}(0)}_{=0} -\nu (L) F = 0
\end{equation}

where we make the underbraced term = 0 by choosing $\nu(0)=0$ in order to satisfy the boundary condition at the fixation. This is the weak form of the equation and is also found by application of the virtual work theorem. The highest differentiation order is here reduced from two to one and allows the first order derivative to be discontinuous. This example does not contain that unless the material property is discontinuous. \\

In fluid mechanics, shocks are discontinuities and consider the following equation in conservation form of a quasi-one dimensional nozzle flow: 

\begin{equation}
\begin{aligned}
\frac{d}{dx} \left( \frac{u^2}{2} \right) - &xu = 0 \qquad u(-1) = 1; \quad u(1) = -0.5\\
&u\frac{du}{dx} = xu  \Leftrightarrow u = \frac{x^2}{2}  + c 
\end{aligned}
\end{equation}

By considering the two boundary conditions the discontinuity appears: 

\begin{equation}
U_L = \frac{x^2 + 1}{L} \qquad U_R = -1 + \frac{x^2}{2}
\end{equation}

The weak form of the previous equation is again obtained using the weighting function $\nu (x)$ that must now vanish at 1 and -1: 

\begin{equation}
\int _{-1}^1 \nu (x)\left[ \frac{d}{dx}\left( \frac{u^2}{2} \right) - xu \right]\, dx = \int _{-1}^{1}\left[\frac{d\nu }{dx} \frac{u^2}{2} - ux \right]\, dx= 0a
\end{equation}

where there is no longer the derivative of $u$. Come back to the bar, an extremum case can be to choose a variation for $\nu(x) = \delta u(x)$ such that: 

\begin{equation}
\int _0 ^L \left[\frac{d\delta u}{dx} k \frac{du}{dx} - \mu g \delta u \right]\, dx  -\delta u (L) F = 0 = \delta \left[ \int _0 ^L \left[\frac{k}{2} \left(\frac{du}{dx}\right) ^2 - \mu g u \right]\, dx  -u(L) F \right]
\end{equation}

This can be interpreted as the variation of the total energy and the solution is the one that gives stationary energy.

\subsection{Shape functions - finite element interpolation}

As explained, shape functions are piecewise polynomial interpolation functions and its parameters are values of the numerical solution in certain points. Here are the principles to construct these shape functions: \\

\begin{itemize}
\item[•] The domain is divided into a set of non-overlapping simple polygons or polyhedra (the edges are not forced to be straight);
\item[•] To each domain we associate some points called \textbf{nodes} and we get an \textbf{element}. In almost all cases, the nodes include the vertices but could also be on the edges or inside the element.; 
\item[•] To each node N is associated a function $\nu_n (x)$ which is defined locally within the element as a polynomial interpolation function. It must vanish at other nodes: 

\begin{equation}
\forall P \in \Omega _e : \nu _n(xp) = \delta _{np}
\end{equation} 

This definition of the interpolation function has several consequences: 

\begin{itemize}
\item The order of the polynomial is directly linked to the number of nodes. For example for a triangular 3 nodes element we will use a linear interpolation in 2 space variables (P1), while for quadratic polynomials 6 nodes are required since we have 6 coefficients in the interpolation (P2). \\

\item The shape function is defined locally for each element. The global basis function associated to a node is thus simply the function equal to the local basis function on each element. If the node belongs to one element the global basis function is the local basis function of the element, if it belongs to several, the global basis function is made of all the local shape function of all the elements it belongs to. A consequence is that the global function associated to node N vanish on all other nodes (called compact support). \\

Also since the basis functions are uniquely defined on each element and since they coincide on a mesh belonging to several element, these are continuous functions. \\

\item The coefficients of the interpolations are values of the numerical solution at the corresponding node: 

\begin{equation}
u^* (x_i) = \sum _{j = 1}^n a_j \nu_j(x_i) = a_i
\end{equation}

We shall call them $a_i = u_i$. \\
\end{itemize}
\end{itemize}


We are going to consider some example in 1D, where we have to divide the the domain into set of intervals and associate nodes. The attribution of nodes differs from polynomial order. 

\paragraph{P1 element}
\wrapfig{5}{l}{5}{0.3}{ch1/8}
First order polynomial has two coefficients ($a + bx$) thus we need two nodes per element to determine them all, we take them on vertices. If we call the local shape functions on element A and B, the global shape function around node $j$ is made of the two.

\begin{equation}
\phi _i^A = \frac{x-x_{j-1}}{x_j-x_{j-1}}\qquad\phi _i^B = \frac{x_{j+1}-x}{x_{j+1}-x_j}
\end{equation}

\paragraph{P2 element}
We have now a 2nd order polynomial ($a + bx + cx^2$) with 3 coefficients and thus 3 nodes are needed. In general the third node is placed at the middle of the element. The shape functions can be simply chosen via the Lagrange interpolation, for example: 

\begin{equation}
\begin{array}{c}
\phi _j ^A = \frac{(x_{j+2} - x)(x_{j+1} - x)}{(x_{j+2} - x_j)(x_{j+1} - x_j)} \qquad \phi _{j+1} ^A = \frac{(x_{j+2} - x)(x - x_j)}{(x_{j+2} - x_{j+1})(x_{j+1}- x_{j})} \\
\phi _{j+2} ^A = \frac{(x_{j+1} - x)(x - x_j)}{(x_{j+1} - x_{j+2})(x_{j+2}- x_{j})}
\end{array}
\end{equation}

or for higher dimensions, using a parent element. The parent element is defined in $\xi , \eta$ for the higher dimension case and in $\xi$ coordinate for 1D where for P2 element the nodes are placed at 0, 1/2 and 1. The shape functions in the normalized coordinates are: 

\begin{equation}
\phi _1 = (2\xi - 1)(\xi -1)\qquad \phi _2= 4\xi (1-\xi)\qquad \phi _3 = \xi (2\xi -1)
\end{equation}

The same shape functions can be used for the transformation, in higher dimension we have: 

\begin{equation}
x(\xi , \eta) = \sum x_i\phi _i (x, y) \qquad y(\xi , \eta) = \sum y_i\phi _i (x, y)
\end{equation}

We speak about \textbf{isoparametric mapping} in this case since the polynomial order is equal to the number of nodes. If lower order polynomials are used we speak of subparametric mapping. 

\paragraph{P3 Hermite element}
\wrapfig{5}{l}{5}{0.3}{ch1/9}
We have $a + bx + cx^2 + dx^3$, we need thus 4 nodes per element. We could just do as before so use Lagrange interpolation with the 2 vertices and two internal nodes, but it is also possible to use Hermite element: we only use the vertices and the numerical parameters are not only solution at nodes but also of its derivative. We have 2 dof at each node so that the shape functions in the normalized element are: 

\begin{equation}
\phi_{0,0} = (2\xi  + 1)(\xi -1)\quad \phi_{0,1} = \xi (\xi -1)^2\quad \phi_{1,0} = (3-2\xi )\xi^2\quad \phi_{1,1} = (\xi - 1)\xi ^2
\end{equation}

In P1 and P2 the basis functions have discontinuous derivatives. The advantage of Hermite is to ensure also the continuity of the derivatives at the nodes. The other advantage is counting the number of dof. Suppose an interval from 0 to L and n elements in the interval, how many unknowns with Lagrange? $\#dof = n+1 + 2 internals = n+1+ 2n = 3n +1 = \mathcal{O}(3n)$ while it is $\mathcal{O}(2n)$ for Hermite. We have the same accuracy at a lower cost. 

\subsection{Discretization - extremum form: Ritz method}
In some cases we do have a variational form, we can find the numerical solution by imposing it to make the variation stationary within the set of numerical solutions of the chosen form: 

\begin{equation}
\frac{\D E (u^h)}{\D u_i} = 0 \qquad i = 1, \dots , n
\end{equation}

We have thus n equations and n unknowns and the solution will be the best in the energy sense. Generally we have something of the form: 

\begin{equation}
\begin{aligned}
E(u) &= \int _\Omega F(u,\nabla u) \, d\Omega + \int _{\D \Omega} g(u) \, d\Gamma \\
\Rightarrow \frac{\D E(u^h)}{\D u_i} &= \int _\Omega \left[ \frac{\D F}{\D u}\frac{\D u^h}{\D u_i} + \frac{\D F}{\D u_{x_p}}\frac{\D}{\D u_i} \left( \frac{\D u^h}{\D x_{p}}\right) \right] \, d\Omega + \int _{\D \Omega} \frac{\D g(u)}{\D u}\frac{\D u^h}{\D u_i} \, d\Gamma = 0\\
&= \int _\Omega \left[ \frac{\D F}{\D u}v _i + \frac{\D F}{\D u_{x_p}}\frac{\D v _i}{\D x_{p}} \right] \, d\Omega + \int _{\D \Omega} \frac{\D g(u)}{\D u}v_i \, d\Gamma = 0
\end{aligned}
\end{equation}

The last line of the equation is obtained by remembering $u^h (x) = \sum _{i=1}^n u_i v_i (x)$. Remark that the last line is the weak form of the equation with $v_i$ as test function. 

\subsection{Discretization - weak form: weighted residual method}
All the differential equations cannot be put under variational form and the previous case is not always applicable. In contrast weak form exists in all case and can be used as basis for discretization. Symbolically if we have $D(u)= 0$, for a numerical solution $u^h$ the equation will not vanish. We will have a residual $D(u^h) = r^h$. It is thus logical to determine the coefficients $u_i$ so that the residual is minimized. The \textbf{least-square} definition consists in minimizing the residual quadratic nom: 

\begin{equation}
J(u^h) = \int _\Omega (r^h)^2 \, d\Omega
\end{equation}
 where for simplicity we considered prescribed solution at the boundary but in general case the boundary condition terms should be added. The minimization consists in: 
 
 \begin{equation}
 \frac{\D J(u^h)}{\D u_i} = \int _\Omega 2\frac{\D r^h}{\D u_i}r^h \, d\Omega = 0 \qquad i = 1,\dots , n
 \end{equation}

the weighting functions are $w_i^{LS} = 2(\D r^h / \D u_i)$, and after integration we find the weak form of the differential equation:

\begin{equation}
\int _\Omega [v F(u, \nabla u) + \nabla v . \vec{g} (u,\nabla u) ] \, d\Omega + \int _{\Gamma = \D \Omega} vH(u) \, d\Gamma = 0
\end{equation}

were we choose $v_i = w_i(x)$. You take as many weighting functions as parameters. Let's enumerate a certain number of requirements the $w_i$ should satisfy: 

\begin{itemize}
\item[•] They should be in the same number of the numerical parameters/shape functions to provide closed algebraic system;
\item[•] They should form a complete set: if you have a domain $\Omega$ the weighting functions should fill the whole domain, no empty space. 
\end{itemize}

Discretization is not unique it depends on the choice of weight. 

\subsubsection{Galerkin method}
The shape functions satisfy the two criteria. If we do that we refind the Ritz method. When a problem can be cast in extremum form, Galerkin method is similar to the Ritz method and is optimal in energy sense. Since the weighting functions belongs to the same functional space than the numerical solution, it requires continuity of the shape functions only up to one order less than that of the highest derivative in the weak form.

\subsubsection{Least squares method}
Has been seen at the beginning of the section. It has the advantage to be tackled by classical minimization methods and to solve the instability problems of the Galerkin method for convection problems. But in contrast it requires $C^1$ continuities for second order derivatives, costly. 

\subsubsection{Point collocation method}
One of the drawbacks of previous methods is to perform computation of comlpex integrals. This can be avoided by using Dirac distributions as weighting functions $w_i (x) = \delta (x-x_i)$ so that the discrete equations become $r^h (x) = 0$. The main disadvantages are that to require a high order approximation and to be less accurate.   

\subsubsection{Subdomain collocation method}
This is a finite volume-like method whereas the previous is a finite element-like method. We choose piecewise constant functions equal to one on a subdomain $\Omega _i$ and zero elsewhere: 

\begin{equation}
\int _{\Omega  _i} r^h \, d\Omega = 0
\end{equation}

This approach offers advantage when the equation to be solved is a conservation law so that the integral can be transformed into surface integral through Gauss theorem. This is known as control-volume based finite element method. The number of subdomains must be equal to the number of nodes. The method is similar to cell vertex centered or vertex centered method since the subdomain is chosen as the set of nodes like them.

\subsubsection{Petrov-Galerkin method}
When it is not one of the previous method and we use several weighting we speak of that. 

\section{Spectral methods}
\subsection{Representation}
Just like finite element, it is based on a functional representation of the solution $u^* (x) = \sum _{i=1}^n a_i v_i (x)$. The difference is that instead of choosing a piecewise polynomial as shape function, we choose trigonometric functions or families of orthogonal functions. For example in lifting line theory it was logical to express $\Gamma (\theta) = \sum {m=1}^N a_m \sin mx$ with $\Gamma (0) = \Gamma (\pi) = 0$, use of truncated Fourrier serie.  As long as the solution is smooth it provides accurate values for few terms in the serie. There are 2 fundamental differences, basic functions are infinitely differentiable, we don’t have to worry about the continuity. In finite elements we had $C^0$ continuity. The convergence is much faster for spectral methods when number of elements increases: FE: $\epsilon \propto h^{k+1}$, spectral: $\epsilon \propto \exp(-1/h)$ . The approximation can be noted as:

\begin{equation}
u^* (x) = \sum _{k = -\frac{n}{2}+1}^{\frac{n}{2}}\hat{u}_k\exp \left( \frac{2\pi Ikx}{L}\right)
\end{equation}

For periodic problems, $v_i$ are trigonometric functions. For non periodic problems we use families of orthogonal polynomials such as Chebyskev, Legendre, Laguerre. Spectral methods is very limited to some problems, it is used for numerical computations around the atmosphere (forecasting).\\

But there is a price to pay for that, the shape functions are not defined locally but globally and the shape functions do not represent the solution value in some certain point. They are non zero over the whole domain, so the system of equations is not sparse, all the parameters are coupled. Basis functions are not of compact support! 

\subsubsection{Discretization}
Same as FE. Since the basis functions are infinitely differentiable we don't have problems with collocations. 


 
